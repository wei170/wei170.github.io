<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Coursera Notes on Guocheng&#39;s Space</title>
    <link>https://wei170.github.io/categories/coursera-notes/</link>
    <description>Recent content in Coursera Notes on Guocheng&#39;s Space</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 03 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://wei170.github.io/categories/coursera-notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Week 5 - Machine Learning</title>
      <link>https://wei170.github.io/blog/coursera/ml/ml-stanford-5/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://wei170.github.io/blog/coursera/ml/ml-stanford-5/</guid>
      <description>Neural Network Cost Function \begin{gather*} \large J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2 \end{gather*}  Some notations:
 L = total number of layers in the network $s_l$ = number of units (not counting bias unit) in layer l K = number of output units/classes  Note:
 The double sum simply adds up the logistic regression costs calculated for each cell in the output layer The triple sum simply adds up the squares of all the individual Î˜s in the entire network.</description>
    </item>
    
    <item>
      <title>Week 4 - Machine Learning</title>
      <link>https://wei170.github.io/blog/coursera/ml/ml-stanford-4/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://wei170.github.io/blog/coursera/ml/ml-stanford-4/</guid>
      <description>Non-linear Hypotheses If create a hypothesis with r polynominal terms from $n$ features, then there will be $\frac{(n+r-1)!}{r!(n-1)!}$. For quadratic terms, the time complexity is $O(n^{2}/2)$. Not pratical to compute.
Neural networks offers an alternate way to perform machine learning when we have complex hypotheses with many features.
Neurons and the Brain There is evidence that the brain uses only one &amp;ldquo;learning algorithm&amp;rdquo; for all its different functions. Scientists have tried cutting (in an animal brain) the connection between the ears and the auditory cortex and rewiring the optical nerve with the auditory cortex to find that the auditory cortex literally learns to see.</description>
    </item>
    
    <item>
      <title>Week 3 - Machine Learning</title>
      <link>https://wei170.github.io/blog/coursera/ml/ml-stanford-3/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://wei170.github.io/blog/coursera/ml/ml-stanford-3/</guid>
      <description>Classification Now we are switching from regression problems to classification problems. Don&amp;rsquo;t be confused by the name &amp;ldquo;Logistic Regression&amp;rdquo;; it is named that way for historical reasons and is actually an approach to classification problems, not regression problems.
Binary Classification Problem y can take on only two values, 0 and 1
Hypothesis Representation We could approach the classification problem ignoring the fact that y is discrete-valued, and use our old linear regression algorithm to try to predict y given x.</description>
    </item>
    
    <item>
      <title>Week 2 - Machine Learning</title>
      <link>https://wei170.github.io/blog/coursera/ml/ml-stanford-2/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://wei170.github.io/blog/coursera/ml/ml-stanford-2/</guid>
      <description>Mutiple Features Linear regression with multiple variables is also known as multivariate linear regression.
The notation for equations:
$$ x_j^{(i)} = \text{value of feature } j \text{ in the }i^{th}\text{ training example} $$
$$ x^{(i)} = \text{the input (features) of the }i^{th}\text{ training example} $$
$$ m = \text{the number of training examples} $$
$$ n = \text{the number of features} $$
The multivariable form of the hypothesis function:</description>
    </item>
    
    <item>
      <title>Week 1 - Machine Learning</title>
      <link>https://wei170.github.io/blog/coursera/ml/ml-stanford-1/</link>
      <pubDate>Thu, 27 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://wei170.github.io/blog/coursera/ml/ml-stanford-1/</guid>
      <description>The Hypothesis Function $$\hat{y} = h_\theta(x) = \theta_0 + \theta_1 x$$
Cost Function To measure the accuracy of the hypothesis function. This takes an average (actually a fancier version of an average) of all the results of the hypothesis with inputs from x&amp;rsquo;s compared to the actual output y&amp;rsquo;s.
$$J(\theta_0, \theta_1) = \dfrac{1}{2m} \displaystyle \sum _{i=1}^m \left( \hat{y}_i- y_i \right)^2 = \dfrac{1}{2m} \displaystyle \sum _{i=1}^m \left (h _\theta(x_i) - y_i \right)^2$$</description>
    </item>
    
  </channel>
</rss>