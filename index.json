[{"categories":["Coursera Notes"],"contents":" Practical aspects of Deep Learning Regularization What we learn in Week 3 is L2 Regularization.\nL1 Regularization is without the square of the $\\theta$.\nImplementation tip: if you implement gradient descent, one of the steps to debug gradient descent is to plot the cost function J as a function of the number of iterations of gradient descent and you want to see that the cost function J decreases monotonically after every elevation of gradient descent with regularization. If you plot the old definition of J (no regularization) then you might not see it decrease monotonically.\nDropout Regularization For reducing overfitting\nImplementing Dropout (illustrate with l = 3, and keep-prob = 0.8):\nd3 = np.random.rand(a3.shape[0], a3.shape[1]) \u0026lt; keep-prob a3 = np.multiple(a3, d3) a3 /= keep-prob  One big downside of drop out is that the cost function J is no longer well-defined. J is not going downhill on very iteration.\n Many of the first successful implementations of drop outs were to computer vision. So in computer vision, the input size is so big, inputting all these pixels that you almost never have enough data. And so drop out is very frequently used by computer vision\n Note: Dropout doesn\u0026rsquo;t work with gradient checking because J is not consistent. You can first turn off dropout (set keep_prob = 1.0), run gradient checking and then turn on dropout again.\nOther regularization methods  Data augmentation  For example in a computer vision data:  You can flip all your pictures horizontally this will give you m more data instances. You could also apply a random position and rotation to an image to get more data.  For example in OCR, you can impose random rotations and distortions to digits/letters. New data obtained using this technique isn\u0026rsquo;t as good as the real independent data, but still can be used as a regularization technique.  Early stopping  In this technique we plot the training set and the dev set cost together for each iteration. At some iteration the dev set cost will stop decreasing and will start increasing. We will pick the point at which the training set error and dev set error are best (lowest training cost with lowest dev cost). We will take these parameters as the best parameters.   Vanishing / Exploding Gradient The level number of deep learning network layers can be large. So if the Ws are just a little bit bigger than one or just a little bit bigger than the identity matrix, then with a very deep network the activations can explode. (i.e. W .^ 100)\nAnd if W is just a little bit less than identity. The activations will decrease exponentially\nWeight Initialization A partial solution to the Vanishing / Exploding gradients in NN is better or more careful choice of the random initialization of weights\nSo, we need the variance which equals $\\frac{1}{n_x}$ to be the range of W\u0026rsquo;s\nHere are three ways to weight initailize $W$:\n For $tanh$ activation: np.random.rand(shape) * np.sqrt(1/n[l-1]) For ReLU: np.random.rand(shape) * np.sqrt(2/n[l-1]) Xavier Initialization: np.random.rand(shape) * np.sqrt(2/(n[l-1] + n[l]))  Gradient checking $$ \\begin{align} \\dfrac{\\partial}{\\partial\\Theta}J(\\Theta) \\approx \\dfrac{J(\\Theta + \\epsilon) - J(\\Theta - \\epsilon)}{2\\epsilon} \\end{align} $$\n$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2}$$ with $\\epsilon = 10^{-7}$ (|| - Euclidean vector norm):\n if it is $\u0026lt; 10^{-7}$ - great, very likely the backpropagation implementation is correct if around $10^{-5}$ - can be OK, but need to inspect if there are no particularly big values in $gradapprox - grad$ if it is $\\geq 10^{-3}$ - bad, probably there is a bug in backpropagation implementation  Optimization Algorithms Mini-batch Gradient Descent In Batch gradient descent we run the gradient descent on the whole dataset.\nWhile in Mini-Batch gradient descent we run the gradient descent on the mini datasets.\nTraining NN with a large data is slow. So we break the data set into mini batches for both $X$ and $Y$ ==\u0026gt; $t: X^{\\{t\\}}, Y^{\\{t\\}}$\nPseudo code:\nfor t = 1:num_of_batches # this is called an epoch AL, caches = forward_prop(X{t}, Y{t}) cost = compute_cost(AL, Y{t}) grads = backward_prop(AL, caches) update_parameters(grads)  Mini-batch size:  (mini batch size = m) ==\u0026gt; Batch gradient descent  Too long per iteration (epoch)  (mini batch size = 1) ==\u0026gt; Stochastic gradient descent (SGD)  Too noisy regarding cost minimization (can be reduced by using smaller learning rate) Don\u0026rsquo;t ever converge (oscelates a lot around the minimum cost) Lose speedup from vectorization  (mini batch size = between 1 and m) ==\u0026gt; Mini-batch gradient descent  Faster learning:  Have the vectorization advantage make progress without waiting to process the entire training set  Doesn\u0026rsquo;t always exactly converge (oscelates in a very small region, but you can reduce learning rate)   Guidelines for choosing mini-batch size  If small training set (\u0026lt; 2000 examples) - use batch gradient descent. It has to be a power of 2 (because of the way computer memory is layed out and accessed, sometimes your code runs faster if your mini-batch size is a power of 2): 64, 128, 256, 512, 1024, \u0026hellip; Make sure that mini-batch fits in CPU/GPU memory. Mini-batch size is a hyperparameter.  Exponentially Weighted Averages There are optimization algorithms that are better than gradient descent, but you should first learn about Exponentially weighted averages.\n$V_t$ is the weighted average for entry t, and $\\theta_t$ is the value for entry t $$ V_t = \\beta V_{t-1} + (1 - \\beta) \\theta_t $$\nIf we plot this it will represent averages over $\\frac{1}{1 - \\beta}$ entries:\n $\\beta = 0.9$ will average last 10 entries $\\beta = 0.98$ will average last 50 entries $\\beta = 0.5$ will average last 2 entries  Intuition:\n Increase $\\beta$, the shift the curve slightly to the right. Decreasing $\\beta$ will create more oscillation within the curve.  Bias correction in exponentially weighted averages When $V_0 = 0$, the bias of the weighted averages is shifted and the accuracy suffers at the start\n$$ V_t = \\frac{\\beta V_{t-1} + (1 - \\beta) \\theta_t}{ 1 - \\beta^t } $$\nNote: As t becomes larger the $1 - \\beta^t$ becomes close to 1\nGradient Descent with Momentum The simple idea is to calculate the exponentially weighted averages for your gradients and then update your weights with the new values.\n$$ \\begin{align} \u0026amp;v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\newline \u0026amp;W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}} \\end{align} $$\nMomentum helps the cost function to go to the minimum point in a more fast and consistent way.\nNote: $\\beta$ is another hyperparameter. $\\beta = 0.9$ is very common and works very well in most cases.\nRMSprop Stands for Root mean square prop.\nRMSprop will make the cost function move slower on the vertical direction and faster on the horizontal direction\n$$ \\begin{align} \u0026amp; s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial J }{\\partial dW^{[l]} })^2 \\newline \u0026amp; W^{[l]} = W^{[l]} - \\alpha \\frac{dW^{[l]}}{\\sqrt{s_{dW^{[l]}}} + \\epsilon} \\end{align} $$\nNotes:\n Name the beta $\\beta_2$ is to differentiate the beta in momentum $\\epsilon$ is used to ensure denominator is not zero  Adam Stands for Adaptive Moment Estimation\nSimply puts RMSprop and momentum together:\n$$ \\begin{align} \u0026amp; v_{W^{[l]}} = \\beta_1 v_{W^{[l]}} + (1 - \\beta_1) \\frac{\\partial J }{ \\partial W^{[l]} } \\newline \u0026amp; v^{corrected}_{W^{[l]}} = \\frac{v_{W^{[l]}}}{1 - (\\beta_1)^t} \\newline \u0026amp; s_{W^{[l]}} = \\beta_2 s_{W^{[l]}} + (1 - \\beta_2) (\\frac{\\partial J }{\\partial W^{[l]} })^2 \\newline \u0026amp; s^{corrected}_{W^{[l]}} = \\frac{s_{W^{[l]}}}{1 - (\\beta_2)^t} \\newline \u0026amp; W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{W^{[l]}}}{\\sqrt{s^{corrected}_{W^{[l]}}}+\\varepsilon} \\end{align} $$\nHyperparameters for Adam:\n Learning rate: needed to be tuned. $\\beta_1$: parameter of the momentum - 0.9 is recommended by default. $\\beta_2$: parameter of the RMSprop - 0.999 is recommended by default. $\\epsilon$: $10^{-8}$ is recommended by default.  Learning rate decay As mentioned before mini-batch gradient descent won\u0026rsquo;t reach the optimum point (converge). But by making the learning rate decay with iterations it will be much closer to it because the steps (and possible oscillations) near the optimum are smaller.\nThree learning rate decay methods:\n$$ \\begin{align} \u0026amp; \\alpha = \\frac{1}{1 + \\text{decay_rate} * \\text{epoch_num}} * \\alpha_0 \\newline \\newline \u0026amp; \\alpha = (0.95 ^ {\\text{epoch_num}}) * \\alpha_0 \\newline \\newline \u0026amp; \\alpha = \\frac{k}{\\sqrt{\\text{epoch_num}}} * \\alpha_0 \\end{align} $$\nThe problem of local optima  The normal local optima is not likely to appear in a dnn because data is usually high dimensional. For point to be a local optima it has to be a local optima for each of the dimensions which is highly unlikely. It\u0026rsquo;s unlikely to get stuck in a bad local optima in high dimensions, it is much more likely to get to the saddle point rather to the local optima, which is not a problem. Plateaus can make learning slow:  Plateau is a region where the derivative is close to zero for a long time. This is where algorithms like momentum, RMSprop or Adam can help.   Hyperparameter tuning, Batch Normalization and Programming Frameworks Tuning Process Hyperparameters are:\n Learning rate. Momentum beta. Mini-batch size. No. of hidden units. No. of layers. Learning rate decay. Regularization lambda. Activation functions. Adam beta1 \u0026amp; beta2.  Its hard to decide which hyperparameter is the most important in a problem. It depends a lot on your problem.\nOne of the ways to tune is to sample a grid with N hyperparameter settings and then try all settings combinations on your problem.\nTry random values: don\u0026rsquo;t use a grid. You can use Coarse to fine sampling scheme:\nWhen you find some hyperparameters values that give you a better performance - zoom into a smaller region around these values and sample more densely within this space.\nThese methods can be automated.\nAppropriate Scale Let\u0026rsquo;s say you have a specific range for a hyperparameter from \u0026ldquo;a\u0026rdquo; to \u0026ldquo;b\u0026rdquo; It\u0026rsquo;s better to search for the right ones using the logarithmic scale rather then in linear scale:\n Calculate: a_log = log(a) # e.g. a = 0.0001 then a_log = -4 Calculate: b_log = log(b) # e.g. b = 1 then b_log = 0 Then:\nr = (a_log - b_log) * np.random.rand() + b_log # In the example the range would be from [-4, 0] because rand range [0,1) result = 10^r   For example, if we want to use the last method on exploring on the \u0026ldquo;momentum beta\u0026rdquo;: * Beta best range is from 0.9 to 0.999. * You should search for 1 - beta in range 0.001 to 0.1 (1 - 0.9 and 1 - 0.999) and the use a = 0.001 and b = 0.1. Then:\na_log = -3 b_log = -1 r = (a_log - b_log) * np.random.rand() + b_log beta = 1 - 10^r # because 1 - beta = 10^r   The reason why randomize $1-\\beta$ instead of $\\beta$ is because $\\frac{1}{1-\\beta}$ is too sensitive when $\\beta$ approches to 1  Pandas vs. Caviar  If you don\u0026rsquo;t have much computational resources you can use the \u0026ldquo;babysitting model\u0026rdquo;. Like Pandas:  Day 0 you might initialize your parameter as random and then start training. Then you watch your learning curve gradually decrease over the day. And each day you nudge your parameters a little during training.  If you have enough computational resources, you can run some models in parallel and at the end of the day(s) you check the results. Like Caviar.  Normalizing Activations In A Network Batch normalization speeds up learning.\nFor any hidden layer, we can normalize $A^{[L]}$ to train $W^{[L]} \\ b^{[L]}$ faster. In practice, normalizing $Z^{[L]}$ before activation.\nAlgorithm:\n$$ \\begin{align} \u0026amp; \\mu = \\frac{1}{m} \\sum_{i}^m Z{[i]} \\newline \u0026amp; \\sigma^2 = \\frac{1}{m} \\sum_{i}^m (Z^{[i]} - \\mu)^2 \\newline \u0026amp; Z_{norm}^{[i]} = \\frac{Z^{[i]} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\newline \u0026amp; \\tilde{Z}^{[i]} = \\gamma Z_{norm}^{[i]} + \\beta \\end{align} $$\nNotes:\n $Z_{norm}^{[i]} $ forces the inputs to a distribution with zero mean and variance of 1. $\\tilde{Z}^{[i]}$ is to make inputs belong to other distribution (with other mean and variance) $\\gamma$ and $\\beta$ are learnable parameters, making the NN learn the distribution of the outputs. If $\\gamma = \\sqrt{\\sigma^2 + \\epsilon}$ and $\\beta = \\mu$ then $\\tilde{Z}^{[i]} = Z_{norm}^{[i]}$  Why does Batch normalization work?  The first reason is the same reason as why we normalize X. The second reason is that batch normalization reduces the problem of input values changing (shifting). Batch normalization does some regularization:  Each mini batch is scaled by the mean/variance computed of that mini-batch. This adds some noise to the values Z[l] within that mini batch. So similar to dropout it adds some noise to each hidden layer\u0026rsquo;s activations. This has a slight regularization effect. Using bigger size of the mini-batch you are reducing noise and therefore regularization effect. Don\u0026rsquo;t rely on batch normalization as a regularization. It\u0026rsquo;s intended for normalization of hidden units, activations and therefore speeding up learning. For regularization use other regularization techniques (L2 or dropout).   Softmax Regression Used for multiclass classification/regression.\nIt is a function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities.\nWe activate softmax regression activation function in the last layer instead of the sigmoid activation.\n$$ S_i = \\frac{e^{Z_i^{[L]}}}{\\sum_{i=i}^K e^{Z_j^{[L]}}} \\text{ for } i = 1, \\ \u0026hellip;, \\ K $$\nTraining a Softmax classifier $$ \\begin{align} \u0026amp; L(y, \\hat{y}) = - \\sum_{j=1}^K y_j \\log{\\hat{y_j}} \\newline \\newline \u0026amp; J(W, b) = - \\frac{1}{m} \\sum_{i=1}^m L(y_i, \\hat{y}_i) \\newline \\newline \u0026amp; dZ^{[L]} = \\hat{Y} - Y \\newline \\newline \u0026amp; DS_i = \\hat{Y} (1 - \\hat{Y}) \\end{align} $$\n","permalink":"https://wei170.github.io/blog/coursera/ml/improve-dnn/","tags":["Machine Learning"],"title":"Additional Note for Improving Deep Neural Network"},{"categories":["Coursera Notes"],"contents":" Deciding What to Try Next Errors in your predictions can be troubleshooted by:\n Getting more training examples Trying smaller sets of features Trying additional features Trying polynomial features Increasing or decreasing $\\lambda$  Model Selection and Train/Validation/Test Sets Test Error $$ J_{test}(\\Theta) = \\dfrac{1}{2m_{test}} \\sum_{i=1}^{m_{test}}(h_\\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2 $$\n Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than any other data set.  Use of the CV set To solve this, we can introduce a third set, the Cross Validation Set, to serve as an intermediate set that we can train d with. Then our test set will give us an accurate, non-optimistic error.\nModel selection:\n Optimize the parameters in $\\Theta$ using the training set for each polynomial degree. Find the polynomial degree d with the least error using the cross validation set. Estimate the generalization error using the test set with $J_{test}(\\Theta^{(d)})$ (d = theta from polynomial with lower error);  Diagnosing Bias vs. Variance The training error will tend to decrease as we increase the degree d of the polynomial.\nAt the same time, the cross validation error will tend to decrease as we increase d up to a point, and then it will increase as d is increased, forming a convex curve.\nRegularization and Bias/Variance  Large $\\lambda$: High bias (underfitting) Intermediate $\\lambda$: just right Small $\\lambda$: High variance (overfitting)  In order to choose the model and the regularization $\\lambda$, we need:\n Create a list of lambdas (i.e. $\\lambda \\in\\{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24\\}$);\n Create a set of models with different degrees or any other variants.\n Iterate through the $\\lambda$s and for each $\\lambda$ go through all the models to learn some $\\Theta$.\n Compute the cross validation error using the learned $\\lambda$ (computed with $\\lambda$) on the $J_{CV}(\\Theta)$ without regularization or $\\lambda = 0$.\n Select the best combo that produces the lowest error on the cross validation set.\n Using the best combo $\\Theta$ and $\\lambda$, apply it on $J_{test}(\\Theta)$ to see if it has a good generalization of the problem.\n  Learning Curves With high bias:\n Low training set size: causes $J_{train}(\\Theta)$ to be low and $J_{CV}(\\Theta)$ to be high.\n Large training set size: causes both $J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ to be high with $J_{train}(\\Theta) \\approx J_{CV}(\\Theta)$.\n  If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.\nWith high variance:\n Low training set size: $J_{train}(\\Theta)$ will be low and $J_{CV}(\\Theta)$ will be high.\n Large training set size: $J_{train}(\\Theta)$ increases with training set size and $J_{CV}(\\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\\Theta) \u0026lt; J_{CV}(\\Theta)$ but the difference between them remains significant.\n  Deciding What to Do Next Revisited    Method Usage     Getting more training examples Fixes high variance   Trying smaller sets of features Fixes high variance   Adding features Fixes high bias   Adding polynomial features Fixes high bias   Decreasing $\\lambda$ Fixes high bias   Increasing $\\lambda$ Fixes high variance    Diagnosing Deep Neural Networks  A neural network with fewer parameters is prone to underfitting. It is also computationally cheaper. A large neural network with more parameters is prone to overfitting. It is also computationally expensive. In this case you can use regularization (increase $\\lambda$) to address the overfitting.  Error Metrics for Skewed Classes It is sometimes difficult to tell whether a reduction in error is actually an improvement of the algorithm.\n For example: In predicting a cancer diagnoses where 0.5% of the examples have cancer, we find our learning algorithm has a 1% error. However, if we were to simply classify every single example as a 0, then our error would reduce to 0.5% even though we did not improve the algorithm.  This usually happens with skewed classes.\nFor this we can use Precision/Recall\nPrecision: $$ \\dfrac{\\text{True Positives}}{\\text{Total number of predicted positives}} = \\dfrac{\\text{True Positives}}{\\text{True Positives}+\\text{False Positives}} $$\nRecall: $$ \\dfrac{\\text{True Positives}}{\\text{Total number of actual positives}} = \\dfrac{\\text{True Positives}}{\\text{True Positives}+\\text{False Negatives}} $$\nBy setting the threshold higher (i.e, $h_\\theta(x) \\geq 0.7$ predict 1), you can get a confident prediction, higher precision but lower recall\nBy setting the threshold lower (i.e, $h_\\theta(x) \\geq 0.3$ predict 1), you can get a safe prediction, higher recall but lower precision\nUse F score to leverage the two metrics: $$\\text{F score} = 2 \\frac{PR}{P+R}$$\n","permalink":"https://wei170.github.io/blog/coursera/ml/ml-stanford-6/","tags":["Machine Learning"],"title":"Week 6 - Machine Learning"},{"categories":["Coursera Notes"],"contents":" Neural Network Cost Function $$ \\begin{gather} J(\\Theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[y^{(i)}_k \\log ((h_\\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\\log (1 - (h_\\Theta(x^{(i)}))_k)\\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} ( \\Theta_{j,i}^{(l)})^2 \\end{gather} $$\nSome notations:\n L = total number of layers in the network $s_l$ = number of units (not counting bias unit) in layer l K = number of output units/classes  Note:\n The double sum simply adds up the logistic regression costs calculated for each cell in the output layer The triple sum simply adds up the squares of all the individual Θs in the entire network. The i in the triple sum does not refer to training example i  Backpropagation Algorithm Our goal is to compute:\n$\\min_\\Theta J(\\Theta)$\nIn this section we\u0026rsquo;ll look at the equations we use to compute the partial derivative of J(Θ):\n$\\dfrac{\\partial}{\\partial \\Theta_{i,j}^{(l)}}J(\\Theta)$\nIn back propagation we\u0026rsquo;re going to compute for every node:\n$\\delta_j^{(l)} = \\text{\u0026ldquo;error\u0026rdquo; of node j in layer l}$\n$a_j^{(l)} = \\text{activation node j in layer l}$\nFor the last layer, we can compute the vector of delta values with:\n$\\delta^{L} = a^{L} - y$\nTo get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:\n$$ \\begin{align} \u0026amp; \\because \u0026amp;\\delta^{(l)} \u0026amp;= ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ g\u0026rsquo;(z^{(l)}) \\newline \u0026amp; \u0026amp; g\u0026rsquo;(u) \u0026amp;= g(u) .* \\ ((1− g(u))) \\newline \\newline \u0026amp; \\therefore \u0026amp;\\delta^{(l)} \u0026amp;= ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .* \\ a^{(l)}\\ .* \\ (1 - a^{(l)}) \\end{align} $$\nWe can compute our partial derivative terms by multiplying our activation values and our error values for each training example t:\n$$ \\begin{align} \\dfrac{\\partial J(\\Theta)}{\\partial \\Theta_{i,j}^{(l)}} = \\frac{1}{m}\\sum_{t=1}^m a_j^{(t)(l)} {\\delta}_i^{(t)(l+1)} \\end{align} $$\nNote: This ignores regularization, which we\u0026rsquo;ll deal with later.\nAlgorithm Given training set $\\lbrace (x^{(1)}, y^{(1)}) \\cdots (x^{(m)}, y^{(m)})\\rbrace$\n Set $\\Delta^{(l)}_{i,j} := 0 \\text{ for all (l)}$\n For training example t = 1 to m, Set $a^{(1)} := x^{(i)}$\n Perform forward propagation to compute $a^{(l)} \\text{ for all l = 2, 3, \u0026hellip;, l}$\n $\\delta^{L} = a^{L} - y$\n Compute $\\delta^{(L - 1)},\\ \\delta^{(L - 2)},\\ \u0026hellip;, \\delta^{(2)}$ using $\\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ a^{(l)}\\ .*\\ (1 - a^{(l)})$\n $\\Delta^{(l)}_{i,j} := \\Delta^{(l)}_{i,j} + a^{(l)}_{j} \\ \\delta^{(l+1)}_{i}$ or with vectorization, $\\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)} \\ (a^{(l)})^T$\n $D^{(l)}_{i,j} := \\dfrac{1}{m}\\left(\\Delta^{(l)}_{i,j} + \\lambda\\Theta^{(l)}_{i,j}\\right) \\text{ if } j \\neq 0$\n $D^{(l)}_{i,j} := \\dfrac{1}{m}\\Delta^{(l)}_{i,j} \\text{ if } j = 0$\n  Unrolling Parameters In order to use optimizing functions such as \u0026ldquo;fminunc()\u0026rdquo;, we will want to \u0026ldquo;unroll\u0026rdquo; all the elements and put them into one long vector:\nthetaVector = [ Theta1(:); Theta2(:); Theta3(:); ] deltaVector = [ D1(:); D2(:); D3(:) ]  If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11, then we can get back our original matrices from the \u0026ldquo;unrolled\u0026rdquo; versions as follows:\nTheta1 = reshape(thetaVector(1:110),10,11) Theta2 = reshape(thetaVector(111:220),10,11) Theta3 = reshape(thetaVector(221:231),1,11)  Gradient Checking Gradient checking will assure that our backpropagation works as intended.\n$$ \\begin{align} \\dfrac{\\partial}{\\partial\\Theta}J(\\Theta) \\approx \\dfrac{J(\\Theta + \\epsilon) - J(\\Theta - \\epsilon)}{2\\epsilon} \\end{align} $$\nWith multiple theta matrices, we can approximate the derivative with respect to $\\Theta_j$as follows:\n$$ \\begin{align} \\dfrac{\\partial}{\\partial\\Theta_j}J(\\Theta) \\approx \\dfrac{J(\\Theta_1, \\dots, \\Theta_j + \\epsilon, \\dots, \\Theta_n) - J(\\Theta_1, \\dots, \\Theta_j - \\epsilon, \\dots, \\Theta_n)}{2\\epsilon} \\end{align} $$\nThe professor Andrew usually uses the value $\\epsilon = 10^{-4}$\nepsilon = 1e-4; for i = 1:n, thetaPlus = theta; thetaPlus(i) += epsilon; thetaMinus = theta; thetaMinus(i) -= epsilon; gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon) end;  Note: Once you\u0026rsquo;ve verified once that your backpropagation algorithm is correct, then you don\u0026rsquo;t need to compute gradApprox again. The code to compute gradApprox is very slow.\nRandomization Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly.\nInstead we can randomly initialize our weights between $[-\\epsilon, \\epsilon]$:\n% If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11. % rand(x,y) will initialize a matrix of random real numbers between 0 and 1 Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON; Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON; Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;  Putting it together First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers total.\n Number of input units = dimension of features $x^{(i)}$ Number of output units = number of classes Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units) Defaults: 1 hidden layer. If more than 1 hidden layer, then the same number of units in every hidden layer.  Training a Neural Network\n Randomly initialize the weights Implement forward propagation to get $h_\\theta(x^{(i)})$ Implement the cost function Implement backpropagation to compute partial derivatives Use gradient checking to confirm that your backpropagation works. Then disable gradient checking. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.  ","permalink":"https://wei170.github.io/blog/coursera/ml/ml-stanford-5/","tags":["Machine Learning"],"title":"Week 5 - Machine Learning"},{"categories":["Coursera Notes"],"contents":" Non-linear Hypotheses If create a hypothesis with r polynominal terms from $n$ features, then there will be $\\frac{(n+r-1)!}{r!(n-1)!}$. For quadratic terms, the time complexity is $O(n^{2}/2)$. Not pratical to compute.\nNeural networks offers an alternate way to perform machine learning when we have complex hypotheses with many features.\nNeurons and the Brain There is evidence that the brain uses only one \u0026ldquo;learning algorithm\u0026rdquo; for all its different functions. Scientists have tried cutting (in an animal brain) the connection between the ears and the auditory cortex and rewiring the optical nerve with the auditory cortex to find that the auditory cortex literally learns to see.\nThis principle is called \u0026ldquo;neuroplasticity\u0026rdquo; and has many examples and experimental evidence.\nModel Representation At a very simple level, neurons are basically computational units that take input (dendrites) as electrical input (called \u0026ldquo;spikes\u0026rdquo;) that are channeled to outputs (axons).\nIn our model, our dendrites are like the input features x1⋯xn, and the output is the result of our hypothesis function:\nIn this model our x0 input node is sometimes called the bias unit. It is always equal to 1.\nOur \u0026ldquo;theta\u0026rdquo; parameters are sometimes instead called \u0026ldquo;weights\u0026rdquo; in the neural networks model.\nActivation function is the smae logistic function.\nWe can have intermediate layers of nodes between the input and output layers called the \u0026ldquo;hidden layer\u0026rdquo;.\n$$ \\begin{align} \u0026amp; a_i^{(j)} = \\text{\u0026ldquo;activation\u0026rdquo; of unit $i$ in layer $j$} \\newline \u0026amp; \\Theta^{(j)} = \\text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$} \\end{align} $$\nFor example, one hidden layer neural network:\n$$ \\begin{bmatrix}x_0 \\newline x_1 \\newline x_2 \\newline x_3\\end{bmatrix} \\rightarrow \\begin{bmatrix}a_1^{(2)} \\newline a_2^{(2)} \\newline a_3^{(2)} \\newline \\end{bmatrix} \\rightarrow h_\\theta(x) \\begin{align} h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_3^{(2)}) \\newline a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3) \\newline a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3) \\newline a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3) \\newline \\end{align} $$\nThe dimensions of these matrices of weights is determined as follows:\n$\\text{If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1} \\times (s_j + 1)$.}$\nActication Functions In the deep learning course provided by deeplearning.ai, sigmoid function is not the only activation function in neural network. Most of the time for hidden units, tanh function performs better than sigmoid function because the values between plus 1 and minus 1, the mean of the activations that come out of your head, and they are closer to having a 0 mean. It kind of has the effect of centering your data so that the mean of your data is closer to 0 rather than, maybe 0.5. And this actually makes learning for the next layer a little bit easier.\nWhile, for the output layer of the binary classification, use the sigmoid function. Other than that, tanh function is always a superior choice.\nReLU is rectifier activation function, and the leaky ReLU: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\nExamples and Intuition Ⅰ $x_1 \\text{ AND } x_2$ is the logical \u0026lsquo;and\u0026rsquo; operator and is only true if both $x_1$ and $x_2$ are 1.\nThe graph of our functions will look like:\n$$ \\begin{align} \\begin{bmatrix} x_0 \\newline x_1 \\newline x_2 \\end{bmatrix} \\rightarrow \\begin{bmatrix} g(z^{(2)}) \\end{bmatrix} \\rightarrow h_\\Theta(x) \\end{align} $$\nNote $x_0$ is the bias variable and is always 1.\nThe first layer thata matrix:\n$$ \\Theta^{(1)} = \\begin{bmatrix}-30 \u0026amp; 20 \u0026amp; 20\\end{bmatrix} $$\nTherefore:\n$$ \\begin{align} \u0026amp; h_\\Theta(x) = g(-30 + 20x_1 + 20x_2) \\newline \\newline \u0026amp; x_1 = 0 \\ \\ and \\ \\ x_2 = 0 \\ \\ then \\ \\ g(-30) \\approx 0 \\newline \u0026amp; x_1 = 0 \\ \\ and \\ \\ x_2 = 1 \\ \\ then \\ \\ g(-10) \\approx 0 \\newline \u0026amp; x_1 = 1 \\ \\ and \\ \\ x_2 = 0 \\ \\ then \\ \\ g(-10) \\approx 0 \\newline \u0026amp; x_1 = 1 \\ \\ and \\ \\ x_2 = 1 \\ \\ then \\ \\ g(10) \\approx 1 \\end{align} $$\nExamples and Intuition Ⅱ The $\\theta$ matrices for AND, NOR, and OR are:\n$$ \\begin{align} AND:\\newline \\Theta^{(1)} \u0026amp;= \\begin{bmatrix}-30 \u0026amp; 20 \u0026amp; 20\\end{bmatrix} \\newline NOR:\\newline \\Theta^{(1)} \u0026amp;= \\begin{bmatrix}10 \u0026amp; -20 \u0026amp; -20\\end{bmatrix} \\newline OR:\\newline\\Theta^{(1)} \u0026amp;= \\begin{bmatrix}-10 \u0026amp; 20 \u0026amp; 20\\end{bmatrix} \\newline \\end{align} $$\nWe can combine these to get the XNOR logical operator (which gives 1 if $x_1$ and $x_2$ are both 0 or both 1).\n$$ \\begin{align} \\begin{bmatrix} x_0 \\newline x_1 \\newline x_2 \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_1^{(2)} \\newline a_2^{(2)} \\end{bmatrix} \\rightarrow \\begin{bmatrix} a^{(3)} \\end{bmatrix} \\rightarrow h_\\Theta(x) \\end{align} $$ $$ \\Theta^{(1)} = \\begin{bmatrix} -30 \u0026amp; 20 \u0026amp; 20 \\newline 10 \u0026amp; -20 \u0026amp; -20 \\end{bmatrix} $$ $$ \\Theta^{(2)} = \\begin{bmatrix} -10 \u0026amp; 20 \u0026amp; 20 \\end{bmatrix} $$\nThe value for all nodes: $$ \\begin{align} \u0026amp; a^{(2)} = g(\\Theta^{(1)} \\cdot x) \\newline \u0026amp; a^{(3)} = g(\\Theta^{(2)} \\cdot a^{(2)}) \\newline \u0026amp; h_\\Theta(x) = a^{(3)} \\end{align} $$\nMulticlass Classification If we want to classify our data into four final resulting classes, (for example classificate the image to cat, dog, bird, or people):\n$$ \\begin{align} \\begin{bmatrix} x_0 \\newline x_1 \\newline x_2 \\newline \\cdots \\newline x_n \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_0^{(2)} \\newline a_1^{(2)} \\newline a_2^{(2)} \\newline \\cdots \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_0^{(3)} \\newline a_1^{(3)} \\newline a_2^{(3)} \\newline \\cdots \\end{bmatrix} \\rightarrow \\cdots \\rightarrow \\begin{bmatrix} h_\\Theta(x)_1 \\newline h_\\Theta(x)_2 \\newline h_\\Theta(x)_3 \\newline h_\\Theta(x)_4 \\newline \\end{bmatrix} \\rightarrow \\end{align} $$\nThen, our resulting hypothesis for one set of inputs looks like: $$ h_\\Theta(x) = \\begin{bmatrix} 0 \\newline 0 \\newline 1 \\newline 0 \\newline \\end{bmatrix} $$\n","permalink":"https://wei170.github.io/blog/coursera/ml/ml-stanford-4/","tags":["Machine Learning"],"title":"Week 4 - Machine Learning"},{"categories":["Coursera Notes"],"contents":" Classification Now we are switching from regression problems to classification problems. Don\u0026rsquo;t be confused by the name \u0026ldquo;Logistic Regression\u0026rdquo;; it is named that way for historical reasons and is actually an approach to classification problems, not regression problems.\nBinary Classification Problem y can take on only two values, 0 and 1\nHypothesis Representation We could approach the classification problem ignoring the fact that y is discrete-valued, and use our old linear regression algorithm to try to predict y given x. However, it is easy to construct examples where this method performs very poorly.\nHypothesis should satisfy:\n$$0 \\leq h_\\theta(x) \\leq 1$$\nSigmoid Function, also called Logistic Function:\n$$ h_\\theta (x) = g ( \\theta^T x ) \\\\ z = \\theta^T x \\\\ g(z) = \\dfrac{1}{1 + e^{-z}} $$\nSigmoid function\n$h_\\theta$ will give us the probability:\n$$ h_\\theta(x) = P(y=1 | x ; \\theta) = 1 - P(y=0 | x ; \\theta) \\\\ P(y = 0 | x;\\theta) + P(y = 1 | x ; \\theta) = 1 $$\nSimplied probability function:\n$$ P(y|x) = h_\\theta(x)^{y} (1 - h_\\theta(x))^{1-y} $$\nSo in order to decease the cost:\n$$ \\begin{align} \\uparrow \\log(P(y|x)) \u0026amp; = \\log(h_\\theta(x)^{y} (1 - h_\\theta(x))^{1-y}) \\newline \u0026amp; = y \\log(h_\\theta(x)) + (1-y) \\log(1 - h_\\theta(x)) \\newline \u0026amp; = - J(\\theta) \\downarrow \\end{align} $$\nDecision Boundary The decision boundary is the line that separates the area where y = 0 and where y = 1.\n$$ \\begin{align} h_\\theta(x) \u0026amp; \\geq 0.5 \\rightarrow y = 1 \\newline h_\\theta(x) \u0026amp; \u0026lt; 0.5 \\rightarrow y = 0 \\newline g(z) \u0026amp; \\geq 0.5 \\quad when \\; z \\geq 0 \\end{align} $$\nSo:\n$$ \\begin{align} \\theta^T x \u0026lt; 0 \u0026amp; \\Rightarrow y = 0 \\newline \\theta^T x \\geq 0 \u0026amp; \\Rightarrow y = 1 \\end{align} $$\nCost Function We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.\nInstead, our cost function for logistic regression looks like:\n$$ \\begin{align} \u0026amp; J(\\theta) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathrm{Cost}(h_\\theta(x^{(i)}),y^{(i)}) \\newline \u0026amp; \\mathrm{Cost}(h_\\theta(x),y) = -\\log(h_\\theta(x)) \\; \u0026amp; \\text{if y = 1} \\newline \u0026amp; \\mathrm{Cost}(h_\\theta(x),y) = -\\log(1-h_\\theta(x)) \\; \u0026amp; \\text{if y = 0} \\end{align} $$\n$J(\\theta)$ vs. $h_\\theta(x)$:\n$$ \\begin{align} \u0026amp; \\text{ if } h_\\theta(x) = y \u0026amp; \\mathrm{Cost}(h_\\theta(x),y) = 0 \\newline \u0026amp; \\text{ if } y = 1 \\; \\mathrm{and} \\; h_\\theta(x) \\rightarrow 0 \u0026amp; \\mathrm{Cost}(h_\\theta(x),y) \\rightarrow \\infty \\newline \u0026amp; \\text{ if } y = 0 \\; \\mathrm{and} \\; h_\\theta(x) \\rightarrow 1 \u0026amp; \\mathrm{Cost}(h_\\theta(x),y) \\rightarrow \\infty \\newline \\end{align} $$\n Simplified Cost Function and Gradient Descent Compress our cost function\u0026rsquo;s two conditional cases into one case:\n$$\\mathrm{Cost}(h_\\theta(x),y) = - y \\; \\log(h_\\theta(x)) - (1 - y) \\log(1 - h_\\theta(x))$$\nEntire cost function:\n$$ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m[y^{(i)} log(h_\\theta(x^{(i)})) + (1 - y^{(i)})log(1 - h_\\theta(x^{(i)}))] $$\nA vectorized implementation is:\n$$ \\begin{align} \u0026amp; h = g(X\\theta)\\newline \u0026amp; J(\\theta) = \\frac{1}{m} \\cdot \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right) \\end{align} $$\nGradient Descent This algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta. $$ \\begin{align} \u0026amp; Repeat \\; \\lbrace \\newline \u0026amp; \\; \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\newline \u0026amp; \\rbrace \\end{align} $$\nIn linear regression $h_\\theta(x) = \\theta^T x$, while in logistic regression $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}$\nA vectorized implementation is:\n$$ \\theta := \\theta - \\frac{\\alpha}{m} X^T (g(X\\theta) - \\vec{y}) $$\nPartial derivative of J(θ) First calculate derivative of sigmoid function:\n$$ \\begin{align} \\sigma(x)\u0026rsquo;\u0026amp;=\\left(\\frac{1}{1+e^{-x}}\\right)\u0026lsquo;=\\frac{-(1+e^{-x}) \u0026lsquo;}{(1+e^{-x})^2}=\\frac{-1\u0026rsquo;-(e^{-x})\u0026lsquo;}{(1+e^{-x})^2}=\\frac{0-(-x)\u0026lsquo;(e^{-x})}{(1+e^{-x})^2}=\\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\\frac{e^{-x}}{(1+e^{-x})^2} \\newline \u0026amp;=\\left(\\frac{1}{1+e^{-x}}\\right)\\left(\\frac{e^{-x}}{1+e^{-x}}\\right)=\\sigma(x)\\left(\\frac{+1-1 + e^{-x}}{1+e^{-x}}\\right)=\\sigma(x)\\left(\\frac{1 + e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}\\right)=\\sigma(x)(1 - \\sigma(x)) \\end{align} $$\nResulting partial derivative:\n$$ \\begin{align} \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \u0026amp;= \\frac{\\partial}{\\partial \\theta_j} \\frac{-1}{m}\\sum_{i=1}^m \\left [ y^{(i)} log (h_\\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\\theta(x^{(i)})) \\right ] \\newline \u0026amp;= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} \\frac{\\partial}{\\partial \\theta_j} log (h_\\theta(x^{(i)})) + (1-y^{(i)}) \\frac{\\partial}{\\partial \\theta_j} log (1 - h_\\theta(x^{(i)}))\\right ] \\newline \u0026amp;= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j} h_\\theta(x^{(i)})}{h_\\theta(x^{(i)})} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 - h_\\theta(x^{(i)}))}{1 - h_\\theta(x^{(i)})}\\right ] \\newline \u0026amp;= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j} \\sigma(\\theta^T x^{(i)})}{h_\\theta(x^{(i)})} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 - \\sigma(\\theta^T x^{(i)}))}{1 - h_\\theta(x^{(i)})}\\right ] \\newline \u0026amp;= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\sigma(\\theta^T x^{(i)}) (1 - \\sigma(\\theta^T x^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{h_\\theta(x^{(i)})} + \\frac{- (1-y^{(i)}) \\sigma(\\theta^T x^{(i)}) (1 - \\sigma(\\theta^T x^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 - h_\\theta(x^{(i)})}\\right ] \\newline \u0026amp;= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} h_\\theta(x^{(i)}) (1 - h_\\theta(x^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{h_\\theta(x^{(i)})} - \\frac{(1-y^{(i)}) h_\\theta(x^{(i)}) (1 - h_\\theta(x^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 - h_\\theta(x^{(i)})}\\right ] \\newline \u0026amp;= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} (1 - h_\\theta(x^{(i)})) x^{(i)}_j - (1-y^{(i)}) h_\\theta(x^{(i)}) x^{(i)}_j\\right ] \\newline \u0026amp;= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} (1 - h_\\theta(x^{(i)})) - (1-y^{(i)}) h_\\theta(x^{(i)}) \\right ] x^{(i)}_j \\newline \u0026amp;= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - y^{(i)} h_\\theta(x^{(i)}) - h_\\theta(x^{(i)}) + y^{(i)} h_\\theta(x^{(i)}) \\right ] x^{(i)}_j \\newline \u0026amp;= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - h_\\theta(x^{(i)}) \\right ] x^{(i)}_j \\newline \u0026amp;= \\frac{1}{m}\\sum_{i=1}^m \\left [ h_\\theta(x^{(i)}) - y^{(i)} \\right ] x^{(i)}_j \\end{align} $$\nMulticlass Classification: One-vs-all $$ \\begin{align} \u0026amp; h_\\theta^{(i)}(x) = P(y = i | x; \\theta) \\quad i \\in {0, 1, \u0026hellip;, n} \\newline \u0026amp; \\mathrm{prediction} = \\max_i( h_\\theta ^{(i)}(x) ) \\end{align} $$\nTo summarize:\n Train a logistic regression classifier $h_\\theta(x)$ for each class￼ to predict the probability that ￼$y = i￼$. To make a prediction on a new x, pick the class ￼that maximizes $h_\\theta(x)$  Regularization Overfitting and Underfitting High bias or underfitting: when the form of our hypothesis function h maps poorly to the trend of the data.\noverfitting or high variance: caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data\nThere are two main options to address the issue of overfitting:\n Reduce the number of features:  Manually select which features to keep. Use a model selection algorithm  Regularization *Keep all the features, but reduce the parameters $\\theta_j$  Regularization works well when we have a lot of slightly useful features.\nRegulated Linear Regression Cost Function If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.\nSay we wanted to make the following function more quadratic:\n$\\theta_0 + \\theta_1x + \\theta_2 x^2 + \\theta_3 x^3 + \\theta_4 x^4$\nTo penalize the influence $\\theta_3x^3$ and $\\theta_4x^4$:\n$min_{\\theta} \\frac{1}{2m} \\sum_{i=1}^m(h_{\\theta}(x^{(i)}) - y^{(i)})^2 + 1000 \\cdot \\theta_3^2 + 1000 \\cdot \\theta_4^2$\nNow, in order for the cost function to get close to zero, we will have to reduce the values of $\\theta_3$ and $\\theta_4$ to near zero, which in turn reduce the values of $\\theta_3x^3$ and $\\theta_4x^4$\nWe could also regularize all of our theta parameters in a single summation:\n$$ min_{\\theta} \\frac{1}{2m} [\\sum_{i=1}^m(h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^n \\theta_j^2] $$\nThe $\\lambda$, or lambda, is the regularization parameter. It determines how much the costs of our theta parameters are inflated. You can visualize the effect of regularization in this interactive plot: https://www.desmos.com/calculator/1hexc8ntqp\nGradient Descent We will modify our gradient descent function to separate out $\\theta_0$ from the rest of the parameters because we do not want to penalize $\\theta_0$\n$$ \\begin{align} \u0026amp; \\text{Repeat}\\ \\lbrace \\newline \u0026amp; \\ \\ \\ \\ \\theta_0 := \\theta_0 - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\newline \u0026amp; \\ \\ \\ \\ \\theta_j := \\theta_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\right] \u0026amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2\u0026hellip;n\\rbrace \\newline \u0026amp; \\rbrace \\end{align} $$\nThe term $\\frac{\\lambda}{m}\\theta_j$ performs our regularization:\n$ \\theta_j := \\theta_j(1 - \\alpha\\frac{\\lambda}{m}) - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n$1 - \\alpha\\frac{\\lambda}{m}$ will always less than 1\nNormal Equation Add in regularization:\n$$ \\begin{align} \u0026amp; \\theta = \\left( X^TX + \\lambda \\cdot L \\right)^{-1} X^Ty \\newline \u0026amp; \\text{where}\\ \\ L = \\begin{bmatrix} 0 \u0026amp; \u0026amp; \u0026amp; \u0026amp; \\newline \u0026amp; 1 \u0026amp; \u0026amp; \u0026amp; \\newline \u0026amp; \u0026amp; 1 \u0026amp; \u0026amp; \\newline \u0026amp; \u0026amp; \u0026amp; \\ddots \u0026amp; \\newline \u0026amp; \u0026amp; \u0026amp; \u0026amp; 1 \\newline \\end{bmatrix} \\end{align} $$\n$L$ is $(n+1)\\times(n+1)$ to exclude $x_0$ with the top left 0.\nRecall that if $m \\leq n$, then $X^TX$ is non-invertible. However, when we add the term $\\lambda \\cdot L$, then $X^TX + lambda \\cdot L$ becomes invertible.\nRegularized Logistic Regression Cost Function $$ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m[y^{(i)} log(h_\\theta(x^{(i)})) + (1 - y^{(i)})log(1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n$\\sum_{j=1}^n \\theta_j^2$ means to explicitly exclude the bias term, $\\theta_0$\nGrediant Descent $$ \\begin{align} \u0026amp; \\text{Repeat}\\ \\lbrace \\newline \u0026amp; \\ \\ \\ \\ \\theta_0 := \\theta_0 - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\newline \u0026amp; \\ \\ \\ \\ \\theta_j := \\theta_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\right] \u0026amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2\u0026hellip;n\\rbrace \\newline \u0026amp; \\rbrace \\end{align} $$\nLooks identical to the gradient descent of the regularized linear regression, but here $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}$ instead of $\\theta^Tx$\n","permalink":"https://wei170.github.io/blog/coursera/ml/ml-stanford-3/","tags":["Machine Learning"],"title":"Week 3 - Machine Learning"},{"categories":["Coursera Notes"],"contents":" Mutiple Features Linear regression with multiple variables is also known as multivariate linear regression.\nThe notation for equations:\n$$ x_j^{(i)} = \\text{value of feature } j \\text{ in the }i^{th}\\text{ training example} $$\n$$ x^{(i)} = \\text{the input (features) of the }i^{th}\\text{ training example} $$\n$$ m = \\text{the number of training examples} $$\n$$ n = \\text{the number of features} $$\nThe multivariable form of the hypothesis function:\n$$ h_\\theta (x) = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n $$\nAssume $$ x_{0}^{(i)} =1 \\text{ for } (i\\in { 1,\\dots, m } )$$\nThen, multivariable hypothesis function can be concisely represented as:\n$$ h_\\theta(x) =\\begin{bmatrix}\\theta_0 \\hspace{2em} \\theta_1 \\hspace{2em} \u0026hellip; \\hspace{2em} \\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\newline x_1 \\newline \\vdots \\newline x_n\\end{bmatrix}= \\theta^T x $$\nGradient Descent for Multiple Variables Repeat until convergence: { $$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\qquad \\text{for j := 0\u0026hellip;n}$$ }\nFeature Scaling and Mean Normalization We can speed up gradient descent by having each of our input values in roughly the same range. This is because $\\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\nFeature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value or the standard deviation) of the input variable, resulting in a new range of just 1.\nMean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.\nSo:\n$$x_{i} := \\frac{x_{i} - \\mu_{i}}{s_{i}}$$\n$\\mu_{i}$ is the average of all values for feature $(i)$ $s_{i}$ is the range of values (max - min), is the standard deviation.\nDebugging Gradient Descent by Learning Rate Make a plot with number of iterations on the x-axis. Now plot the cost function, $J(\\theta)$ over the number of iterations of gradient descent. If $J(\\theta)$ ever increases, then you probably need to decrease $\\alpha$.\nIf $\\alpha$ is too small: slow convergence. If $\\alpha$ is too large: ￼may not decrease on every iteration and thus may not converge.\nPolynominal Regression We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).\nNormal Equation $$\\theta = (X^TX)^{-1}X^Ty$$\nA method of finding the optimum theta without iteration. No need to do feature scaling with the normal equation.\nProof of the normal equation\n   Gradient Descent Normal Equation     Need to choose alpha No need to choose alpha   Needs many iterations No need to iterate   $O(kn^2)$ $O(n^3)$ need to calculate $(X^TX)^{-1}$   Works well when n is large Slow if n is very large    Noninvertability If $X^TX$ is noninvertible, the common causes might be having :\n Redundant features, where two features are very closely related (i.e. they are linearly dependent)  Too many features (e.g. m ≤ n). In this case, delete some features or use \u0026ldquo;regularization\u0026rdquo; (to be explained in a later lesson).  When implementing the normal equation in octave we want to use the \u0026lsquo;pinv\u0026rsquo; function rather than \u0026lsquo;inv.\u0026rsquo;\n","permalink":"https://wei170.github.io/blog/coursera/ml/ml-stanford-2/","tags":["Machine Learning"],"title":"Week 2 - Machine Learning"},{"categories":["Coursera Notes"],"contents":" The Hypothesis Function $$\\hat{y} = h_\\theta(x) = \\theta_0 + \\theta_1 x$$\nCost Function To measure the accuracy of the hypothesis function. This takes an average (actually a fancier version of an average) of all the results of the hypothesis with inputs from x\u0026rsquo;s compared to the actual output y\u0026rsquo;s.\n$$J(\\theta_0, \\theta_1) = \\dfrac{1}{2m} \\displaystyle \\sum _{i=1}^m \\left( \\hat{y}_i- y_i \\right)^2 = \\dfrac{1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h _\\theta(x_i) - y_i \\right)^2$$\nBreak it apart:\n $h_\\theta (x_i) - y_i$ is the difference between the predicted value and the actual value.\n$\\bar{x}$ is the mean of all $\\left (h_\\theta (x_i) - y_i \\right)^2$\n$J(\\theta_0, \\theta_1) = \\frac{1}{2}\\bar{x} $\n The function is also called Square Error Function, or Mean squared error\nThe mean is halfed as a convenience for the computation of the gradient descent, as the derivative of the square function will cancel out the $\\frac{1}{2}$ term\nIntuition Ⅰ To simplify the visualization of the cost function, assume $\\theta_0$ is 0, which means the cost function is $J(\\theta_1)$\nIntuition Ⅱ If the $\\theta_0$ is not 0, then the contour plot will look like this:\nIf projected onto a 2d plot\nKey features:\n Every point on the same \u0026lsquo;circle\u0026rsquo; has the same $J(\\theta_0, \\theta_1)$ The center of the inner most \u0026lsquo;circle\u0026rsquo; is the minimal cost function value.  Gradient Descent To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.\nThe way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter α, which is called the learning rate.\nAs shown above, two points are next to each other, but their local minimal cost function value are different.\nThe gradient descent algorithm is:\n repear until convergence: $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)$$ where $j = 0,1$ represents the feature index number\n At each iteration j, one should simultaneously update the parameters $\\theta_0, \\theta_1, \\theta_2, \u0026hellip;$. (Simultaneous Update)\nIntuition Ⅰ Simplied gradient descent with $\\theta_0$ is 0:\n repear until convergence: $$\\theta_1 := \\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_1} J(\\theta_1)$$\n When the slope is negative, the value of $\\theta_1$ ​increases.\nWhen it is positive, the value of $\\theta_1$ decreases.\nIntuition Ⅱ We should adjust our parameter $\\alpha$ to ensure that the gradient descent algorithm converges in a reasonable time.\nFailure to converge or too much time to obtain the minimum value imply that our step size is wrong.\nIntuition Ⅲ Gradient descent can converge to a local minimum, even with the learning rate is fixed.\nWhen approaching the local minimum, slope value is smaller, and so the step size is smaller.\nGradient Descent For Linear Regression   repeat until convergence:\u0026#xA0; {      \u0026#x03B8; 0  :=    \u0026#x03B8; 0  \u0026#x2212; \u0026#x03B1;  1 m   \u0026#x2211; i = 1  m   (  h \u0026#x03B8;  (  x i   ) \u0026#x2212;  y i   )      \u0026#x03B8; 1  :=    \u0026#x03B8; 1  \u0026#x2212; \u0026#x03B1;  1 m   \u0026#x2211; i = 1  m    (  h \u0026#x03B8;  (  x i   ) \u0026#x2212;  y i   )  x i         }     Batch gradient descent: looks at every example in the entire training set on every step\n","permalink":"https://wei170.github.io/blog/coursera/ml/ml-stanford-1/","tags":["Machine Learning"],"title":"Week 1 - Machine Learning"},{"categories":["Project Overview"],"contents":" Overview I named this project DCenter for Drone Center, hoping that it can develop into a centralized platform that support commercial drones to fly half-autonomously with collision avoidance and stream data and 4K video to any user.\nCurrently, I setup a simplified infrastructure on the Heroku and GCP, and build an Android app as both the adaptor and the controller of the drone. Cellular Drone blog will give you a more comprehensive view of the whole project and the evaluation of the current stage.\nDemo The following is a quick demo of the project. There are two Android phones. The right one works as the Drone Receptro (DR), which is connected to the drone via the micro-usb. The left one serves as the Remote Controller (RC), which send all control messages to the cloud server.\nIn order to simplify the shooting, I used DJI Assistant Simulator to visualize the movement of the drone.\nAt the end of the video, I will show you the website and the end-to-end latency of each control message.\n  Indexes I break down the development into 5 parts:\n DCenter Dev Part Ⅰ: GraphQL API DCenter Dev Part Ⅱ: Server and Deployment DCenter Dev Part Ⅲ: Drone Receptor DCenter Dev Part Ⅳ: Remote Controller DCenter Dev Part Ⅴ: Network Analysis  ","permalink":"https://wei170.github.io/blog/cellular-drone-development/","tags":["Demo","Project Breakdown"],"title":"Overview of The Cellular Drone Development"},{"categories":["Project Report"],"contents":" Motivation Over the past few years, drones have become central to the functions of various businesses and governmental organizations and have managed to pierce through areas where certain industries were either stagnant or lagging behind.\nHowever, almost all the commercial drones are controlled with the remote controller via the WiFi connection, which highly restricts the scope of the mobility and scale of discovery. In addition, the WiFi network is not secure. The attacker can easily disconnects the controller by changing the SSID and immediately reconnects and is then able to transmit commands though the malicious drone to the hijacked drone.\nBesides, there are no centralized platform like other IoT devices to monitor and control multiple devices at the same time. All commercial drones are controlled in VLOS, Visual Line-of-Sight. In the future, drones need to have the ability to free-up pilots and go Beyond VLOS. A centralized platform can achieve this easily no matter the drone is autonomous or non-autonomous.\nAlso, the industry demands of the drone will not be limited to entertainment and photography. Live streaming video, package delivery and rescue mission will push the drones to be connected to the cellular network in order to free up their mobility.\nProblem If aerial vehicles are connected to the cellular network, the first application will be the Beyond-VLOS control. BVLOS needs more sensors and cameras equipped on the drone. The reason is that the first-view camera is not enough to monitor the status of the drone. More sensors and cameras can enhance the accuracy and improve the detection of their surroundings. But it will introduced more data in the network. At the same time, the increase of amount of data should not lag the control speed.\nSecond concern is the collision avoidance system. BVLOS drones should have the ability to communicate with nearby drones to avoid collision. If the latency is high and drones cannot react in short period of time, accidents may happen.\nThe last concern is that in the future 5G network, the benchmark for the latency cellular drones is 10ms. It is hard to achieve in current LTE network.\nSo this project will not only focus on the development of the cellular drone application, but also analyze the end-to-end latency in order to achieve the low-latency requirement. The breakdown analysis can help us to observe the bottleneck and reveal the weakest section in this project.\nSolution I break down the entire development into three main parts.\n Android App  Drone Receptor Remote Controller  Server  Host Database (i.e. Drone Status, User, Flight Control Messages, etc.) Handle requests  Website  Show data Quick mutation   Android app serves two main purpose: one is the Drone Receptor (DR) that is directly connected to the drone with a micro-usb, listens to any control message sent to the drone, and sends updates of the drone status to the server; the other is the Remote Controller (RC) that is on the hand of the pilot, sends flight control messages to the server, and listens to any update from the drone.\nChallenge Based on the experiments I did before and researches I read, higher altitude and faster speed will introduce higher latency and higher rate of packet loss. Furthermore, due to the interference of neighbor base stations, the rate of attach request failure will increase, which indicates that the handoff events take longer to complete.\nSo I decided to run experiments at the maximum 10m in height to decrease the influence of these issues and to make sure the flight failure cannot create any accident.\nArchitecture Figure 1: Architecture Diagram\nSpecs  DJI Phantom 4 Pro v2.0\n DJI Android SDK\n GCP\n GCP hosts my server in the cloud. I use Cloud DNS to lower latency and increase scalability, Cloud Load Balancing to scale the application on Google Compute Engine from zero to full-throttle with no pre- warming needed, and a instance group to host a kubernetes container connecting to a PostgreSQL Cloud SQL server.\n PostgreSQL\n GraphQL\n The reason why I chose GraphQL api instead of REST api is because of latency. REST api uses multiple endpoints, and each endpoint will introduce latency. GraphQL api solves this problem by combining and integrating multiple endpoints into one. So it can decrease the end-to-end latency.\n Apollo\n Apollo is the framework of GraphQL that can help me to develop the Android app easier.\n Android\n AT\u0026amp;T\n I run all experiments in the AT\u0026amp;T network.\n  Cellular Drone Project Development is the blog focused on the details of the development. If you are interested in how I implemented all these, go check it out.\nNetwork Communication As shown in the Figure 1, RC and DR communicate with the server with both HTTP and Websocket protocols. One situation is when the pilot user is controlling on the RC, flight control messagess will be sent to the server through the HTTP protocol. When the server receive any message, it will publish the message through the Websocket protocol to the subscribing DR. The other situation is when the status of the drone is updated, DR will send the updates through the HTTP protocol. When the serve receive any update, it will publish the updated status to the listening RC.\nLatency Analysis Overall Latency Figure 2: Overall Latency\nThis project is mainly focused on the end-to-end latency of the flight control message. So the difference between the time when a flight control message is generated on the RC and the time when the flight control message is received by the DR and executed on the drone. Therefore, there are three main factors of the latency: overhead on both phones, network latency, and overhead on the server.\nI run the experiment for 10 rounds. However, 7 of them have valid and complete data. The boxplot in Figure 2 shows the median is around 500ms and the interquartile range is from 150ms to 650ms. That number is not adequate to the requirement.\nIn Figure 3, we can see that the latency can decrease down to 113ms and increase to 950ms. So we need to breakdown each latency to expose the bottleneck.\nFigure 3: Every Round End-to-End Latency\nBreakdown The latency will be breakdown based on the three main factors mentioned before. After the breakdown, we will visualize which section is the root cause of the high latency.\nFigure 4: Latency Breakdown\nFigure 4 is the breakdown of one experiment. It clearly shows that the server overhead is the obstacle of the overall latency. The RC overhead, HTTP RRT and WS latency are all very stable and low. The DR overhead is stable most of the time, below 10ms, but can be over 300ms in some edge cases due to the execution time of the drone.\nFuture Work Drawbacks There are some main drawbacks in my architecture. The first is the two-hops communication between the RC and DR. If we want to reduce the latency, less hops should be the main thing to focus on. The second is the overhead on the server. Because it upholds more than 50% of the latency. In the future, I need to investigate the causes of the overhead and fix them. The third is the network latency. Unfortunately, Google is not a network provider. Therefore, reducing the RRT is not easy to achieve.\nDevelopment Live streaming video is another essential feature in the commercial drone industry. The DR is able to upload streaming video packets to the server and the server will publish the video packets to the RCs. Therefore, latency, uplink performance and downlink performance will be the KPIs.\nBesides, collision avoidance system is also important. It can be divided into two parts: collision avoidance with nearby drones and with static objects. For the first part, the server is responsible to find groups of drones that are near with each other and send them immediate flight control messages to avoid the possible accident. At the same time, send warning notification messages to RCs that are in control of the drones and switch the mode to autonomous. The second part could be achieved by training convolutional neural networks of visual recognition and implementing it on the RCs so that the drones can react to the objects immediately with low latency.\nThanks! Photo by Jacob Owens on Unsplash\n","permalink":"https://wei170.github.io/blog/cellular-drone/","tags":["Cellular Network","MobileInsight","LTE","AT\u0026T","Latency","Drone","DJI","Aerial Vehicle","Android","GraphQL","Apollo","Cloud Service","GCP"],"title":"Cellular Drone"}]